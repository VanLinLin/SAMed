{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from pycocotools.coco import COCO\n",
    "from segment_anything import sam_model_registry as sam_model_registry_lora\n",
    "from segment_anything_new import SamPredictor, sam_model_registry\n",
    "from segment_anything_new.utils.transforms import ResizeLongestSide\n",
    "from SurfaceDice import compute_dice_coefficient\n",
    "from pathlib import Path\n",
    "from tqdm.auto import tqdm\n",
    "# set seeds\n",
    "torch.manual_seed(1234)\n",
    "np.random.seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create image list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.02s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "task = 'test'\n",
    "root_dir = Path(rf'D:\\Side_project\\SAMed\\data\\map_1-1_gastrointestinal_coco\\{task}')\n",
    "anno_info = COCO(rf'D:\\Side_project\\SAMed\\data\\map_1-1_gastrointestinal_coco\\{task}\\_annotations.coco.json')\n",
    "\n",
    "def get_image(image_id):\n",
    "    image_info = anno_info.loadImgs(image_id)[0]\n",
    "    image_path = root_dir / image_info['file_name']\n",
    "    ann_ids = anno_info.getAnnIds(imgIds=image_id)\n",
    "    anns = anno_info.loadAnns(ann_ids)\n",
    "\n",
    "    mask = anno_info.annToMask(anns[0])\n",
    "    \n",
    "    image_data = np.array(Image.open(root_dir / image_info['file_name']))\n",
    "\n",
    "    def get_bbox_from_mask(mask):\n",
    "        '''Returns a bounding box from a mask'''\n",
    "        y_indices, x_indices = np.where(mask > 0)\n",
    "        x_min, x_max = np.min(x_indices), np.max(x_indices)\n",
    "        y_min, y_max = np.min(y_indices), np.max(y_indices)\n",
    "        # add perturbation to bounding box coordinates\n",
    "        H, W = mask.shape\n",
    "        x_min = max(1, x_min - np.random.randint(0, 20))\n",
    "        x_max = min(W-1, x_max + np.random.randint(0, 20))\n",
    "        y_min = max(1, y_min - np.random.randint(0, 20))\n",
    "        y_max = min(H-1, y_max + np.random.randint(0, 20))\n",
    "\n",
    "        return np.array([x_min, y_min, x_max, y_max])\n",
    "\n",
    "    # gt_data = io.imread(join(ts_gt_path, test_names[img_idx]))\n",
    "    bbox_raw = get_bbox_from_mask(mask)\n",
    "    # print(f'{bbox_raw=}')\n",
    "\n",
    "\n",
    "    # # preprocess: cut-off and max-min normalization\n",
    "    lower_bound, upper_bound = np.percentile(image_data, 0.5), np.percentile(image_data, 99.5)\n",
    "    image_data_pre = np.clip(image_data, lower_bound, upper_bound)\n",
    "    image_data_pre = (image_data_pre - np.min(image_data_pre))/(np.max(image_data_pre)-np.min(image_data_pre))*255.0\n",
    "    image_data_pre[image_data==0] = 0\n",
    "    image_data_pre = np.uint8(image_data_pre)\n",
    "    H, W, _ = image_data_pre.shape\n",
    "\n",
    "    return image_data_pre, bbox_raw, mask, H, W\n",
    "# # predict the segmentation mask using the original SAM model\n",
    "# ori_sam_predictor.set_image(image_data_pre)\n",
    "# ori_sam_seg, _, _ = ori_sam_predictor.predict(point_coords=None, box=bbox_raw, multimask_output=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create SAM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MedSAM model\n",
    "ori_sam_model= sam_model_registry['vit_b'](checkpoint=r\"D:\\Side_project\\SAMed\\MedSAM\\medsam_vit_b.pth\")\n",
    "\n",
    "ori_sam_model.to(device)\n",
    "\n",
    "ori_sam_predictor = SamPredictor(ori_sam_model)\n",
    "\n",
    "# Fine tune MedSAM model with mask decoder\n",
    "checkpoint_gas = r'D:\\Side_project\\MedSAM\\work_dir\\demo2D_ICIP\\sam_model_best.pth'\n",
    "sam_model= sam_model_registry['vit_b'](checkpoint=checkpoint_gas)\n",
    "\n",
    "sam_model.to(device)\n",
    "\n",
    "# Fine tune MedSAM model with LoRA\n",
    "from importlib import import_module\n",
    "\n",
    "checkpoint_sam = r'D:\\Side_project\\SAMed\\MedSAM\\medsam_vit_b.pth'\n",
    "checkpoint_lora = r'D:\\Side_project\\SAMed\\output\\MedSAM\\results\\gastrointestinal_512_pretrain_vit_b_epo200_bs5_lr0.0001\\best_epoch=102_valid_loss=0.068.pth'\n",
    "sam_model, _ = sam_model_registry_lora['vit_b'](image_size=512, \n",
    "                                            num_classes=2,\n",
    "                                            checkpoint=checkpoint_sam, \n",
    "                                            pixel_mean=[0, 0, 0],\n",
    "                                            pixel_std=[1, 1, 1])\n",
    "\n",
    "sam_model.to(device)\n",
    "\n",
    "pkg = import_module('sam_lora_image_encoder')\n",
    "net = pkg.LoRA_Sam(sam_model, 4).cuda()\n",
    "net.load_lora_parameters(checkpoint_lora)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 396/396 [03:00<00:00,  2.19it/s]\n"
     ]
    }
   ],
   "source": [
    "ori_sam_dsc_list = []\n",
    "tuned_sam_mask_decoder_list = []\n",
    "lora_list = []\n",
    "for image_id in tqdm(range(len(list(root_dir.glob('*.png'))))):\n",
    "    image_data_pre, bbox_raw, mask, H, W = get_image(image_id)\n",
    "\n",
    "    ## Original MedSAM predict\n",
    "    ori_sam_predictor.set_image(image_data_pre)\n",
    "    ori_sam_seg, _, _ = ori_sam_predictor.predict(point_coords=None, box=bbox_raw, multimask_output=False)\n",
    "    ori_sam_dsc = compute_dice_coefficient(mask>0, ori_sam_seg>0)\n",
    "    ori_sam_dsc_list.append(ori_sam_dsc)\n",
    "\n",
    "\n",
    "    ## Fine tune MedSAM with mask decoder predict\n",
    "    sam_transform = ResizeLongestSide(sam_model.image_encoder.img_size)\n",
    "    resize_img = sam_transform.apply_image(image_data_pre)\n",
    "    # print(resize_img.shape)\n",
    "    # plt.imshow(resize_img)\n",
    "\n",
    "    resize_img_tensor = transforms.ToTensor()(resize_img).to(device)\n",
    "    # print(resize_img_tensor.shape)\n",
    "    # resize_img_tensor = torch.as_tensor(resize_img.transpose(2, 0, 1)).to(device)\n",
    "\n",
    "    input_image = sam_model.preprocess(resize_img_tensor.unsqueeze(dim=0)) # (1, 3, 1024, 1024)\n",
    "    # assert input_image.shape == (1, 3, sam_model.image_encoder.img_size, sam_model.image_encoder.img_size), 'input image should be resized to 1024*1024'\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # pre-compute the image embedding\n",
    "        ts_img_embedding = sam_model.image_encoder(input_image)\n",
    "        # convert box to 1024x1024 grid\n",
    "        bbox = sam_transform.apply_boxes(bbox_raw, (H, W))\n",
    "        # print(f'{bbox_raw=} -> {bbox=}')\n",
    "        box_torch = torch.as_tensor(bbox, dtype=torch.float, device=device)\n",
    "        if len(box_torch.shape) == 2:\n",
    "            box_torch = box_torch[:, None, :] # (B, 4) -> (B, 1, 4)\n",
    "        \n",
    "        sparse_embeddings, dense_embeddings = sam_model.prompt_encoder(\n",
    "            points=None,\n",
    "            boxes=box_torch,\n",
    "            masks=None,\n",
    "        )\n",
    "        medsam_seg_prob, _ = sam_model.mask_decoder(\n",
    "            image_embeddings=ts_img_embedding.to(device), # (B, 256, 64, 64)\n",
    "            image_pe=sam_model.prompt_encoder.get_dense_pe(), # (1, 256, 64, 64)\n",
    "            sparse_prompt_embeddings=sparse_embeddings, # (B, 2, 256)\n",
    "            dense_prompt_embeddings=dense_embeddings, # (B, 256, 64, 64)\n",
    "            multimask_output=False,\n",
    "            )\n",
    "        medsam_seg_prob = torch.sigmoid(medsam_seg_prob)\n",
    "\n",
    "        pred_shapped_masks = F.interpolate(\n",
    "            medsam_seg_prob, # shape: (1, 1, 256, 256)\n",
    "            (224, 224),\n",
    "            mode=\"bilinear\",\n",
    "            align_corners=False,\n",
    "        )\n",
    "\n",
    "        # convert soft mask to hard mask\n",
    "        pred_shapped_masks = pred_shapped_masks.cpu().numpy().squeeze()\n",
    "        medsam_seg = (pred_shapped_masks > 0.5).astype(np.uint8)\n",
    "    medsam_dsc = compute_dice_coefficient(mask>0, medsam_seg>0)\n",
    "    tuned_sam_mask_decoder_list.append(medsam_dsc)\n",
    "\n",
    "    ### Fine tune MedSAM with LoRA predict\n",
    "    transform = transforms.Compose([transforms.ToPILImage(),\n",
    "                                transforms.Resize(size=(512,512)),\n",
    "                                transforms.ToTensor()])\n",
    "    img_tensor = transform(image_data_pre)\n",
    "    img_tensor = img_tensor.unsqueeze(dim=0).to(device)\n",
    "    with torch.no_grad():\n",
    "        output = net(img_tensor, False, 512)\n",
    "    transform = transforms.Compose([transforms.Resize(size=(224,224))])\n",
    "\n",
    "    output_masks = torch.argmax(torch.softmax(output['masks'], dim=1), dim=1, keepdim=True)\n",
    "    output_masks = output_masks.squeeze(dim=0) * 50\n",
    "    output_masks = transform(output_masks)\n",
    "    output_masks_np_lora = output_masks.cpu().numpy()\n",
    "    output_masks_np_lora = (output_masks_np_lora > 0).astype(np.uint8)\n",
    "    sam_lora_dsc = compute_dice_coefficient(mask>0, output_masks_np_lora>0)\n",
    "    lora_list.append(sam_lora_dsc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean dice of Original MedSAM: 0.7393\n",
      "Mean dice of Tuned MedSAM with mask decoder: 0.4291\n",
      "Mean dice of Tuned MedSAM with LoRA: 0.3022\n"
     ]
    }
   ],
   "source": [
    "print(f'Mean dice of Original MedSAM: {sum(ori_sam_dsc_list) / len(ori_sam_dsc_list):.4f}')\n",
    "print(f'Mean dice of Tuned MedSAM with mask decoder: {sum(tuned_sam_mask_decoder_list) / len(tuned_sam_mask_decoder_list):.4f}')\n",
    "print(f'Mean dice of Tuned MedSAM with LoRA: {sum(lora_list) / len(lora_list):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean dice of Original MedSAM: 0.6442\n",
      "Mean dice of Tuned MedSAM with mask decoder: 0.2445\n",
      "Mean dice of Tuned MedSAM with LoRA: 0.5942\n"
     ]
    }
   ],
   "source": [
    "print(f'Mean dice of Original MedSAM: {sum(ori_sam_dsc_list) / len(ori_sam_dsc_list):.4f}')\n",
    "print(f'Mean dice of Tuned MedSAM with mask decoder: {sum(tuned_sam_mask_decoder_list) / len(tuned_sam_mask_decoder_list):.4f}')\n",
    "print(f'Mean dice of Tuned MedSAM with LoRA: {sum(lora_list) / len(lora_list):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
